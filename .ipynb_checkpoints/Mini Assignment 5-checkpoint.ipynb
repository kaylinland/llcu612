{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen/1818 Northanger Abbey.txt', 'austen/1815 Emma.txt', 'austen/1811 Sense and Sensibility.txt', 'austen/1805 Lady Susan.txt', 'austen/1813 Pride and Prejudice.txt', 'austen/1790 Love And Freindship.txt', 'austen/1814 Mansfield Park.txt', 'austen/1818 Persuasion.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob(\"austen/*txt\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"austen/1815 Emma.txt\", \"r\")\n",
    "textString = f.read()\n",
    "f.close()\n",
    "\n",
    "with open(\"austen/1815 Emma.txt\", \"r\") as f: \n",
    "    emmaString = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = emmaString.find(\"Emma\")#search for beginning of text\n",
    "end = emmaString.find(\"FINIS\")#search for end of text\n",
    "emmaText = emmaString[start:end]#create a string with all text\n",
    "import nltk\n",
    "austentokens = nltk.word_tokenize(emmaText)#use tokenizer to create tokenized version of string\n",
    "austentokens[:10]#display 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'She', 'was', 'the']\n",
      "        and         the        with          to          of        Emma   Woodhouse    handsome      clever        rich           a comfortable        home       happy disposition      seemed       unite        some        best   blessings \n",
      "          3           3           2           2           2           1           1           1           1           1           1           1           1           1           1           1           1           1           1           1 \n"
     ]
    }
   ],
   "source": [
    "emmaTextsample = [word for word in austentokens[:50] if word[0].isalpha()]#remove white space\n",
    "print(emmaTextsample)\n",
    "emmafreqdist = nltk.FreqDist(emmaTextsample)\n",
    "emmafreqdist.tabulate(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've succesfully imported my text, converted \"Emma\" to a string and used NLTK to examine work frequency and create a concordance. I have discovered that by changing the width and lines of the concordance command, I can display more words on either side of the search term and also search larger or smaller portions of text. I'm not going to work through creating a regex concordance because I already did that in my last mini assignment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Emma Woodhouse handsome clever and rich with a...>\n",
      "Displaying 1 of 1 matches:\n",
      " handsome clever and rich with a comfortable \n"
     ]
    }
   ],
   "source": [
    "concordancetext = nltk.Text(emmaTextsample)\n",
    "print(concordancetext)\n",
    "concordancetext.concordance(\"handsome\", width=100, lines=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Voyant, I have identified the three most common terms using the Links tool in Emma. These are \"Mrs\", \"said\" and \"Mr\". As \"Mrs.\" and \"Mr.\" are both connected to character names (as well as said in most cases), the terms that are most often collocated with these titles may not prove that illuminating for analysis. However, it could prove useful in seeing which characters are most frequently mentioned in the tex, as well as for ensuring word counts of certain character names include all references to that name (both with and without titles). Using hte Terms Berry tool, I note that some of the \"berries\" do not show up with text until they are clicked on. I think this is a technical issue. Now I will work with NLTK to examine collocates within Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Emma', 'Woodhouse'),\n",
       " ('Woodhouse', 'handsome'),\n",
       " ('handsome', 'clever'),\n",
       " ('clever', 'rich'),\n",
       " ('rich', 'comfortable'),\n",
       " ('comfortable', 'home'),\n",
       " ('home', 'happy'),\n",
       " ('happy', 'disposition'),\n",
       " ('disposition', 'seemed'),\n",
       " ('seemed', 'unite')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "austenwords = [word for word in austentokens if word[0].isalpha() and word.lower() not in stopwords]#remove whitespace and make lower case\n",
    "from nltk import bigrams, trigrams #import collocate tools\n",
    "tokens = austenwords\n",
    "list(bigrams(tokens))[:10]#list first 10 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Mr.', 'Knightley'), 0.003685064488628551), (('Mrs.', 'Weston'), 0.0033688089541566977), (('Mr.', 'Elton'), 0.0028875505321343123), (('Miss', 'Woodhouse'), 0.00231004042570745), (('Mr.', 'Weston'), 0.0022000385006737616), (('Frank', 'Churchill'), 0.0019387839287187526), (('Mrs.', 'Elton'), 0.0019387839287187526), (('Mr.', 'Woodhouse'), 0.0018012815224266424), (('Miss', 'Fairfax'), 0.0016637791161345323), (('every', 'thing'), 0.0015950279129884773)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import * #import packages with measures for collocates\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "BigramCollectionFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "finder = BigramCollectionFinder.from_words(austenwords) #input Austen text into finder\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)#create list of bigrams and show frequencies\n",
    "print(scored[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK, I can identify all bigrams within \"Emma\" and create a list of those bigrams. This would probably not prove to be all that useful, as it is just a list of all the words in the text as they appear next to each other. To try to gain some more information, I imported the bigram measures tool which allows me to print the bigrams with their frequencies. This might prove more useful for analysis and more directly aligns with what Voyant can show about the text. I can also use the \"sorted\" function to sort bigrams in order of their relative frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Box', 'Hill'),\n",
       " ('Colonel', 'Campbell'),\n",
       " ('Emma', 'Harriet'),\n",
       " ('Emma', 'could'),\n",
       " ('Emma', 'felt'),\n",
       " ('Emma', 'found'),\n",
       " ('Emma', 'said'),\n",
       " ('Emma', 'thought'),\n",
       " ('Emma', 'would'),\n",
       " ('Every', 'body'),\n",
       " ('Frank', 'Churchill'),\n",
       " ('Harriet', 'Smith'),\n",
       " ('Harriet', 'could'),\n",
       " ('Harriet', 'would'),\n",
       " ('Jane', 'Fairfax'),\n",
       " ('John', 'Knightley'),\n",
       " ('Knightley', 'could'),\n",
       " ('Maple', 'Grove'),\n",
       " ('Miss', 'Bates'),\n",
       " ('Miss', 'Campbell'),\n",
       " ('Miss', 'Fairfax'),\n",
       " ('Miss', 'Hawkins'),\n",
       " ('Miss', 'Nash'),\n",
       " ('Miss', 'Smith'),\n",
       " ('Miss', 'Taylor'),\n",
       " ('Miss', 'Woodhouse'),\n",
       " ('Mr', 'Elton'),\n",
       " ('Mr', 'Knightley'),\n",
       " ('Mr.', 'Churchill'),\n",
       " ('Mr.', 'Cole'),\n",
       " ('Mr.', 'Dixon'),\n",
       " ('Mr.', 'Elton'),\n",
       " ('Mr.', 'Frank'),\n",
       " ('Mr.', 'John'),\n",
       " ('Mr.', 'Knightley'),\n",
       " ('Mr.', 'Martin'),\n",
       " ('Mr.', 'Mrs.'),\n",
       " ('Mr.', 'Perry'),\n",
       " ('Mr.', 'Weston'),\n",
       " ('Mr.', 'Woodhouse'),\n",
       " ('Mrs.', 'Bates'),\n",
       " ('Mrs.', 'Churchill'),\n",
       " ('Mrs.', 'Cole'),\n",
       " ('Mrs.', 'Dixon'),\n",
       " ('Mrs.', 'Elton'),\n",
       " ('Mrs.', 'Goddard'),\n",
       " ('Mrs.', 'John'),\n",
       " ('Mrs.', 'Weston'),\n",
       " ('Oh', 'Miss'),\n",
       " ('Oh', 'dear'),\n",
       " ('Oh', 'yes'),\n",
       " ('Robert', 'Martin'),\n",
       " ('Upon', 'word'),\n",
       " ('Woodhouse', 'would'),\n",
       " ('body', 'else'),\n",
       " ('could', 'bear'),\n",
       " ('could', 'help'),\n",
       " ('could', 'never'),\n",
       " ('could', 'possibly'),\n",
       " ('could', 'think'),\n",
       " ('could', 'wish'),\n",
       " ('cried', 'Emma'),\n",
       " ('dare', 'say'),\n",
       " ('dear', 'Emma'),\n",
       " ('dear', 'Jane'),\n",
       " ('dear', 'Miss'),\n",
       " ('dear', 'sir'),\n",
       " ('every', 'body'),\n",
       " ('every', 'day'),\n",
       " ('every', 'thing'),\n",
       " ('glad', 'see'),\n",
       " ('good', 'deal'),\n",
       " ('great', 'deal'),\n",
       " ('great', 'pleasure'),\n",
       " ('half', 'hour'),\n",
       " ('last', 'night'),\n",
       " ('much', 'better'),\n",
       " ('much', 'love'),\n",
       " ('much', 'obliged'),\n",
       " ('must', 'go'),\n",
       " ('next', 'day'),\n",
       " ('one', 'could'),\n",
       " ('poor', 'Harriet'),\n",
       " ('quite', 'well'),\n",
       " ('replied', 'Emma'),\n",
       " ('said', 'Emma'),\n",
       " ('said', 'Mr.'),\n",
       " ('said', 'Mrs.'),\n",
       " ('shall', 'never'),\n",
       " ('soon', 'afterwards'),\n",
       " ('take', 'care'),\n",
       " ('thought', 'would'),\n",
       " ('would', 'much'),\n",
       " ('would', 'never'),\n",
       " ('would', 'rather'),\n",
       " ('would', 'soon'),\n",
       " ('young', 'ladies'),\n",
       " ('young', 'lady'),\n",
       " ('young', 'man'),\n",
       " ('young', 'woman')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(finder.nbest(bigram_measures.raw_freq, 100))#display top 10 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top 100 collocations, one can see some interesting collocations in addition to character names and titles. The repeated usage of \"could\" and \"would\" might prove useful for analysis, as well as the frequent usage of \"dear\" in conjunction with character names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking in terms of the future and how I could expand this line of inquiry, I think I find the concatenation tool to be more instinctively useful than the collocate tool. I am not sure I yet understand the best way to utilize the collocate tool to provide a starting place for meaningful analysis, so I would be interested in reading some studies that make use of this tool. I have seen some interesting applications in language learning with collocates, where collocates are used to help students of foreign languages identify particular word pairs that occur frequently in a particular language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
